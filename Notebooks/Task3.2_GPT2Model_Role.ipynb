{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Task3.2_GPT2Model_Role.ipynb","provenance":[{"file_id":"https://github.com/MuchenZhang/cooking-AI/blob/master/Copy_of_Train_a_GPT_2_Text_Generating_Model_w_GPU.ipynb","timestamp":1616653277598}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GT6TfUq4-_xR"},"source":["## IMPORTS"]},{"cell_type":"code","metadata":{"id":"KBkpRgBCBS2_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618122273590,"user_tz":-330,"elapsed":5550,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"915db11f-f511-4381-a146-ec23c0ab25d8"},"source":["%tensorflow_version 1.x\n","!pip install -q gpt-2-simple\n","import gpt_2_simple as gpt2\n","from datetime import datetime\n","from google.colab import files\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: pipreqs in /usr/local/lib/python3.7/dist-packages (0.4.10)\n","Requirement already satisfied: yarg in /usr/local/lib/python3.7/dist-packages (from pipreqs) (0.1.9)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from pipreqs) (0.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yarg->pipreqs) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2.10)\n","ERROR: Failed on file: /content/drive/MyDrive/TestFastAPI_v2.py\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pipreqs\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 470, in main\n","    init(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 409, in init\n","    follow_links=follow_links)\n","  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 138, in get_all_imports\n","    raise exc\n","  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 124, in get_all_imports\n","    tree = ast.parse(contents)\n","  File \"/usr/lib/python3.7/ast.py\", line 35, in parse\n","    return compile(source, filename, mode, PyCF_ONLY_AST)\n","  File \"<unknown>\", line 1\n","    !pip install uvicorn fastapi\n","    ^\n","SyntaxError: invalid syntax\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sUmTooTW3osf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618122005813,"user_tz":-330,"elapsed":51460,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"3db12ebb-8063-470d-f7c9-3dcf4d1657f6"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sun Apr 11 06:20:05 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P8wSlgXoDPCR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618122014212,"user_tz":-330,"elapsed":59850,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"5f593d96-0113-46ba-c34e-53875297a6fb"},"source":["gpt2.download_gpt2(model_name=\"124M\")\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Fetching checkpoint: 1.05Mit [00:00, 338Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 4.33Mit/s]\n","Fetching hparams.json: 1.05Mit [00:00, 498Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 70.2Mit/s]                                  \n","Fetching model.ckpt.index: 1.05Mit [00:00, 259Mit/s]                                                \n","Fetching model.ckpt.meta: 1.05Mit [00:00, 5.87Mit/s]\n","Fetching vocab.bpe: 1.05Mit [00:00, 9.94Mit/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4eWTdBIPWjY","executionInfo":{"status":"ok","timestamp":1618122014213,"user_tz":-330,"elapsed":59842,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"6bb50a9a-a77d-4f7d-bb64-732c617576ad"},"source":["gpt2.mount_gdrive()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t0m-BNx__IT8"},"source":["## Load Data set"]},{"cell_type":"code","metadata":{"id":"6OFnPCLADfll","executionInfo":{"status":"ok","timestamp":1618122014214,"user_tz":-330,"elapsed":59841,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["file_name = \"role_dataset.txt\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"Gyo6pjZvF6J7","executionInfo":{"status":"ok","timestamp":1618122020036,"user_tz":-330,"elapsed":65656,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"98fca02d-2c49-4227-f672-b1fc1a9b509c"},"source":["netflix_data = pd.read_excel('/content/drive/My Drive/JobDescriptionPrediction/Resources/Data/processed/roles_dataset.xlsx')\n","netflix_data[['JobID','Job_Title','Output']].head(n=2)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>JobID</th>\n","      <th>Job_Title</th>\n","      <th>Output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9</td>\n","      <td>bioinformatics data scientist</td>\n","      <td>['when applying, we ask that you reference any...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13</td>\n","      <td>data scientist</td>\n","      <td>['so if that gets you excited, apply!, role ex...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   JobID  ...                                             Output\n","0      9  ...  ['when applying, we ask that you reference any...\n","1     13  ...  ['so if that gets you excited, apply!, role ex...\n","\n","[2 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"WoDjNVNo_Wms","executionInfo":{"status":"ok","timestamp":1618122020036,"user_tz":-330,"elapsed":65654,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["TRAIN_SIZE      = 0.8\n","def split_data(netflix_data, S=TRAIN_SIZE):\n","    print(TRAIN_SIZE)\n","\n","    # Split into training and validation sets    \n","    train_size = int(S * len(netflix_data))\n","\n","\n","    train_data = netflix_data[:train_size]\n","    val_data = netflix_data[train_size:]\n","\n","    return train_data, val_data"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"oLqTtPqu_Xiv","executionInfo":{"status":"ok","timestamp":1618122020037,"user_tz":-330,"elapsed":65644,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"124ca7f9-9486-48e3-f663-eaf038d5b3b6"},"source":["train_data, val_data = split_data(netflix_data, TRAIN_SIZE)\n","\n","f'There are {len(train_data) :,} samples for training, and {len(val_data) :,} samples for validation testing'"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0.8\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'There are 1,417 samples for training, and 355 samples for validation testing'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"A1ehXL15G9Fm","executionInfo":{"status":"ok","timestamp":1618122020038,"user_tz":-330,"elapsed":65642,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["train_data.to_csv('role_dataset.txt', header=True, index=False, sep='\\t', mode='w')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z6okFD8VKtS","executionInfo":{"status":"ok","timestamp":1618122020039,"user_tz":-330,"elapsed":65641,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["\n","# gpt2.copy_file_from_gdrive(\"/content/drive/My Drive/JobDescriptionPrediction/Resources/Data/processed/role_dataset.txt\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdpZQXknFNY3"},"source":["## Finetune GPT-2\n"]},{"cell_type":"code","metadata":{"id":"aeXshJM-Cuaf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617286447698,"user_tz":-330,"elapsed":1336476,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"4e5b9f13-1d93-4bd1-f070-e181d80e39e2"},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.finetune(sess,\n","              dataset=file_name,\n","              model_name='124M',\n","              steps=1000,\n","              restore_from='fresh',\n","              run_name='run2',\n","              print_every=10,\n","              sample_every=200,\n","              save_every=100\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Loading checkpoint models/124M/model.ckpt\n","INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loading dataset...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["dataset has 787996 tokens\n","Training...\n","[10 | 18.89] loss=3.32 avg=3.32\n","[20 | 31.33] loss=3.28 avg=3.30\n","[30 | 43.77] loss=3.26 avg=3.28\n","[40 | 56.19] loss=2.96 avg=3.20\n","[50 | 68.63] loss=2.89 avg=3.14\n","[60 | 81.06] loss=3.15 avg=3.14\n","[70 | 93.49] loss=2.99 avg=3.12\n","[80 | 105.93] loss=2.83 avg=3.08\n","[90 | 118.35] loss=2.70 avg=3.04\n","[100 | 130.78] loss=2.69 avg=3.00\n","Saving checkpoint/run2/model-100\n","[110 | 145.88] loss=2.48 avg=2.95\n","[120 | 158.32] loss=3.06 avg=2.96\n","[130 | 170.77] loss=2.71 avg=2.94\n","[140 | 183.19] loss=2.71 avg=2.92\n","[150 | 195.64] loss=2.71 avg=2.91\n","[160 | 208.08] loss=2.75 avg=2.90\n","[170 | 220.51] loss=3.02 avg=2.91\n","[180 | 232.94] loss=2.77 avg=2.90\n","[190 | 245.37] loss=2.48 avg=2.87\n","[200 | 257.81] loss=2.15 avg=2.83\n","Saving checkpoint/run2/model-200\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","======== SAMPLE 1 ========\n","istical, predictive, sentiment analysis and data modeling a plus but there is a requirement to have some hands on experience with sql etl framework a plus but ideally it is possible to communicate with various teams in an effective manner a plus but it is very important you are comfortable with python experience is a plus, we strongly recommend google gm jira 2 a plus a plus, google provides a high level of customer service and innovation every day and we are an equal opportunity employer.]']\n","1073\tdata scientist vp experience\tbusiness intelligence \t['this role is designed to provide technical and business development support in a fast paced and highly-competitive environment, as we are looking to develop an innovative platform, business model, and analytical infrastructure.', 'this position will be responsible for overseeing the creation, enhancement, and deployment of our various product offerings using statistical modeling techniques and data mining techniques.', 'our core responsibilities include:,    work closely with various teams across the company and will help to define the business requirements of our product;   assist in developing new and innovative software and solutions to drive adoption, build, and maintain strong internal and external client relationships using machine learning, regression, clustering, hypothesis testing, and predictive analysis;   help develop and operate our product and our analytics products,   design, develop, and administer data driven and automated testing,   coordinate and administer daily project delivery, and drive ad hoc data analytics and marketing dashboards;   facilitate and coordinate multiple key team meetings and conferences on various topics.', ', #li-mm1]']\n","1075\tdata scientist ii\tdata mining r sql\t['[the following role will be involved in design, develop, develop, and manage a team of data scientists with over 100+ years of experience in the field of health, biotechnology, bioinformatics, or related analytics.', 'this role is responsible for evaluating a team of data science team members and or scientists to identify new tools, techniques, and initiatives required to build, or enhance our product suite or business model.,    apply machine learning and biopharmaceutical analytical techniques   help design new and exciting clinical machine-learning algorithms to mine patient data   recommend cutting edge machine learning products or clinical trials in order to understand the disease and treatment landscape   help design and implement and optimize algorithms that use real scientific data from other sources   help partner with cross-functional teams to develop and deploy highly optimized algorithms for solving complex clinical trials   be a part of a project team led by a clinical biotherm   lead clinical trials to evaluate new, experimental and or experimental-scale drug development or clinical trial   lead or lead a cross-functional team to develop and deploy algorithms for solving complex clinical trials   partner with cross-functional projects to analyze, document and present new research findings   lead small-scale clinical trials   assist in the development and deployment of new algorithms that are relevant for the clinical trial research,    bachelor degree in biomedical, behavioral biology, medical field, or equivalent clinical setting   strong analytical problem solving   experience analyzing large, complex data   proficient in relational, database languages and sql to solve complex problem solving   experience communicating results of deep analysis to business partners   excellent oral and written communication skills   demonstrated experience with scientific methodologies, statistical methods and methodologies   demonstrated expertise in data management and data quality   excellent team player, strong organizational, decision-making and communication skills   experience in clinical machine-learning products and products   background with healthcare industry is a plus.,    primary location,    preferred locations,    specialty,    primary locations,    primary locations,    education levels,    education levels,    primary location,    preferred locations,    additional preferred locations,    we are looking for an innovative, agile, innovative candidate, with an ability to solve complex business problems with innovative and exciting results, with outstanding team work, passion and a dedication to our mission.', 'we are confident in a person who is passionate about our mission and a team with a wide variety of projects we can focus on.', ', the following is an extract from the job report and related internal documentation at americanpowerbase.org.', 'all jobs are evaluated on a grade-by-grade basis.', ', please see the full job report overview for details and qualifications.', ', our role & responsibilities,    identify, develop and evaluate a team of data scientists   develop and develop algorithms to mine patient data   recommend cutting edge machine learning and biopharmaceutical analytical techniques   lead small-scale clinical trials using real scientific data from other sources   help partner with cross-functional teams to develop and deploy algorithms for solving complex clinical trials   assist in the development and deployment of new algorithms that are relevant for the clinical trial research,    bachelor degree in biomedical, behavioral biology\n","\n","[210 | 281.22] loss=2.21 avg=2.80\n","[220 | 293.64] loss=2.42 avg=2.78\n","[230 | 306.08] loss=2.50 avg=2.77\n","[240 | 318.52] loss=2.25 avg=2.74\n","[250 | 330.97] loss=2.46 avg=2.73\n","[260 | 343.41] loss=2.40 avg=2.72\n","[270 | 355.85] loss=2.64 avg=2.71\n","[280 | 368.28] loss=2.42 avg=2.70\n","[290 | 380.73] loss=2.41 avg=2.69\n","[300 | 393.19] loss=1.96 avg=2.66\n","Saving checkpoint/run2/model-300\n","[310 | 407.73] loss=2.60 avg=2.66\n","[320 | 420.18] loss=2.53 avg=2.65\n","[330 | 432.64] loss=2.15 avg=2.64\n","[340 | 445.07] loss=2.20 avg=2.62\n","[350 | 457.51] loss=2.11 avg=2.60\n","[360 | 469.95] loss=2.02 avg=2.59\n","[370 | 482.38] loss=2.41 avg=2.58\n","[380 | 494.83] loss=2.13 avg=2.57\n","[390 | 507.29] loss=1.83 avg=2.54\n","[400 | 519.72] loss=2.17 avg=2.53\n","Saving checkpoint/run2/model-400\n","======== SAMPLE 1 ========\n"," the business of this position.]']\n","4228\tdata analyst \ttime management microsoft sql server sql ai sentiment analytics tableau project-based computing tableau microsoft access microsoft access power bi qlikview sas tableau project-based computing microsoft access splunk qlikview tableau big data big data spark hadoop shell scripting kafka mongodb\t['[the analyst leads the data analytics and research activities; leads a team of analysts and data analysts responsible for building and refining ai’s products and services; contributes to global innovation and business insights and supports efforts from state to enhance our ecommerce offerings in adtech, c3, and digital infrastructure; and performs key data analysis and interpretation of data to support strategy and recommendations for the business.', 'expertise under the supervision of the company's marketing and product manager in the areas of product management and consumer, business, and consumer insights.', 'supports product management as it relates to the ability of data to guide product decisions and process.', 'participates in project planning, business projects, and product presentations and product releases to gather business requirements and provide recommendations to product leadership to ensure quality.', ', required qualifications , advanced master’s degree in a business analytics or data wrangling program (data acquisition, data governance, machine learning, business machine learning) required, if you have previous experience using or planning to use data, you must have a minimum of five years of experience in a data analytics or data wrangling role.', 'previous experience with bi, machine learning or statistical modeling software required, and proven experience in bi, machine learning, analysis or web development required.', 'experience with advanced analytics packages such as google analytics, adobe analytics etc.', 'have a strong ability to analyze and interpret business information to improve decision making and decision making processes and decisions.', 'excellent communication and organizational skills, with the ability to work cross-dependently with others who do not have this background.', 'ideally, we would also have a highly experienced associate degree, but in the event you are in need of an advanced degree, or a combination of education and experience, you will be on your own.', 'strong hands-on skills in business analysis, data analysis, design and analysis, reporting, visualization, presentation, and reporting required.', ', required qualifications , ba bs or ms in business analysis, business intelligence, data analysis, economics, finance, mathematics, finance, statistics, operations research, or related field with 3+ years experience in an analytical role working with large amount of bi and machine learning data   bi, analytics and or web development work with bi and ecommerce business lines, or with data mining and web development projects, and collaborate across end to end with business intelligence and data scientists to make recommendations; provide project planning and projects design; lead research and analysis to meet critical business business issues; collaborate with engineering, design and or coding teams; assist with the production of data reports, key performance indicators and metrics; design and prepare presentations to management and management of bi and machine learning data analysis and data analysis; conduct research and analysis on complex data sets, identify trends, patterns, or other relevant patterns from the data; develop the data platform and tools to leverage information from large array of sources to drive business development, design product, and project needs; assist with data aggregation, data mapping, and data quality assessment; develop automated queries, reports, dashboards, and other systems that monitor project readiness and performance and provide meaningful insights; analyze, interpret, and communicate business cases and insights to senior management; develop and deliver training content to include presentations; review and recommend appropriate techniques and business applications to meet business needs.', 'provide support to technology and technical teams for data management, data quality, and business unit initiatives; oversee the quality and availability of data products and service streams for support; participate in technical development and testing activities, provide technical oversight for data products, maintain policies for data quality and data integration; serve as an on-call liaison between business unit, engineering, information technology, and departmental teams, providing support to all data-related tasks related to this position; act as a resource for technical and business support staff (technical, business and customer experts) who need support in the implementation and implementation of data products and services and for the customer benefit and support needs of the company for the development and implementation of data products and services; develop and develop bi tools, scripts, scripts, etc.', 'to be used in the development and testing of data products and services; implement and manage processes related to the performance of bi tools, scripts, etc.', 'to be used in the development and testing of bi and data tools (e.g., bi-data warehouse, or bi-taskshelf, or bi-customer) for various data analytic purposes; assist in the use of bi tools, scripts, etc.', 'to be used in the development and testing of bi and data tools\n","\n","[410 | 541.86] loss=1.90 avg=2.51\n","[420 | 554.31] loss=2.24 avg=2.50\n","[430 | 566.74] loss=2.22 avg=2.50\n","[440 | 579.17] loss=2.38 avg=2.49\n","[450 | 591.60] loss=2.21 avg=2.49\n","[460 | 604.03] loss=2.24 avg=2.48\n","[470 | 616.45] loss=1.93 avg=2.46\n","[480 | 628.89] loss=1.91 avg=2.45\n","[490 | 641.31] loss=2.09 avg=2.44\n","[500 | 653.73] loss=1.83 avg=2.43\n","Saving checkpoint/run2/model-500\n","[510 | 668.33] loss=1.65 avg=2.41\n","[520 | 680.77] loss=1.65 avg=2.39\n","[530 | 693.23] loss=1.41 avg=2.36\n","[540 | 705.67] loss=2.01 avg=2.36\n","[550 | 718.10] loss=2.02 avg=2.35\n","[560 | 730.53] loss=2.15 avg=2.34\n","[570 | 742.97] loss=1.83 avg=2.33\n","[580 | 755.43] loss=2.04 avg=2.32\n","[590 | 767.86] loss=1.52 avg=2.31\n","[600 | 780.30] loss=1.58 avg=2.29\n","Saving checkpoint/run2/model-600\n","======== SAMPLE 1 ========\n","facisance and experience]']\n","3496\tassociate vp business intelligence\tjava natural language processing data warehouse management experience informatica\t['ability to use sql for large-scale data analysis and to build data sources and or models and to communicate data outputs to all customer support teams including technical experts.,    duties and responsibilities,    as a data analyst or engineer you will be responsible for:,    preparing tables, graphs, and charts using relational databases such as oracle databases and sql.', 'analyzing complex data assignments using business objects such as tables.', 'working with data engineers to prepare data sets for data analysis.', 'working with data to debug issues.', 'training data engineers on data integration, etl, and data migration.', 'training data engineers on data documentation.', 'training data engineers on using analytics tools to solve data problems.', 'training data engineers on applying behavioral approaches to data analysis.', 'training data engineers on writing sql queries into large-scale databases and oracle databases.', 'training data engineers on building data models and or models that are scalable, reliable, and reproducible.', 'identify, train, and mentor high performing data analysts.', 'partner with other data teams to develop end-to-end data science projects.', 'collaborate with different teams in order to develop high performing projects.', 'manage the creation of data models, data aggregation, or data flow models.', 'partner with data engineering teams to formulate data analysis plans.', 'proactively share knowledge with team members.', 'provide leadership in the development of data science projects.', 'build and enhance data sources and automate algorithms to support future data science projects.', 'manage data warehouse and database setup.', 'build and manage custom data models, databases, and etl to support next-generation data science projects.', 'develop and test web apps using adobe analytics.', 'work with data engineering teams to support new mobile initiatives.', 'perform data quality check and ensure data and or web analytics are up-to-date.', ', required competencies,    3-5 years of experience working with database technologies and relational databases.', 'bachelor’s degree in statistics with a focus on behavioral analysis and data science.', 'minimum 5-7 years using statistical software as a client service tool.', 'minimum 3-5 years in biopharmaceutical data science.', 'minimum 3-5 years in working with data and or bioinformatics projects.', 'minimum 2-5 years using data visualization software such as tableau, d3, ggplot, etc.', 'experience creating data warehouse and data model for web applications.', 'data mining to extract structured and unstructured data for scientific and academic study.', 'knowledge and experience in applying statistical analysis and data mining techniques to extract relevant data within and within the context of academic or clinical research.', 'excellent oral and written communication skills and ability to explain complex methods to non-experts.', 'excellent problem solving skills.', 'strong interpersonal skills, both verbal and written.', 'willingness to communicate complex topics with an understanding of both academic and clinical practice.', 'able to distill and communicate complex ideas into simple administrative data analysis using data visualization software.', 'ability to explain complex topics to non-experts.', 'experience designing data reporting processes.', 'previous experience using databases or working with it to create data models.', 'past experience working in etl, microsoft excel experience using statistical modeling software (r, sas, t-sql, etc.)', 'and estatistical software experience is preferred.', 'highly motivated and capable of working as part of a team and working with others.]']\n","3501\tdata analyst ii\tdata mining machine learning r sas oracle sql\t['[ the primary focus for this role is in virtualization of the product   enterprise infrastructure (stonemasons   enterprise solutions) within a large enterprise (mft) environment.,    the role:,    you will drive the focus of our core solutions in the enterprise, including new logical logical volumes (folio) with access to traditional hard drives, new technologies in optical media center, new enterprise platforms (eoe logical blocks) using open standards (e.g., eos), and enterprise data center (ec2) using the latest open standards (e.g., nfs).,    you will also drive the work of the vps team on enterprise architecture and upgrades, both onshore and offshore, working to enhance and standardize vps capabilities, including the customer and vendor analytics of the enterprise.', 'you will also drive the execution of our customer centric project across the enterprise, addressing technical resources to identify and prioritize business and technical opportunities.', ', qualifications,    bachelor’s degree\n","\n","[610 | 802.50] loss=1.55 avg=2.27\n","[620 | 814.96] loss=1.84 avg=2.26\n","[630 | 827.40] loss=2.00 avg=2.26\n","[640 | 839.84] loss=2.24 avg=2.26\n","[650 | 852.27] loss=1.69 avg=2.25\n","[660 | 864.70] loss=1.50 avg=2.23\n","[670 | 877.14] loss=1.79 avg=2.22\n","[680 | 889.57] loss=1.82 avg=2.21\n","[690 | 901.99] loss=1.63 avg=2.20\n","[700 | 914.42] loss=1.46 avg=2.19\n","Saving checkpoint/run2/model-700\n","[710 | 929.04] loss=1.65 avg=2.18\n","[720 | 941.46] loss=1.44 avg=2.16\n","[730 | 953.90] loss=2.05 avg=2.16\n","[740 | 966.33] loss=1.97 avg=2.16\n","[750 | 978.77] loss=1.18 avg=2.14\n","[760 | 991.21] loss=1.83 avg=2.13\n","[770 | 1003.65] loss=1.46 avg=2.12\n","[780 | 1016.08] loss=1.18 avg=2.10\n","[790 | 1028.51] loss=1.37 avg=2.09\n","[800 | 1040.94] loss=1.56 avg=2.08\n","Saving checkpoint/run2/model-800\n","======== SAMPLE 1 ========\n"," a data analysis and visualization experience in ml azure solutions.', 'excellent written and oral communication skills are required.', 'proficiency in excel and access (preferably tableau but available from google office) in tableau.', 'excellent time management skills, using excel to organize work for frequent travel.', 'demonstrated ability to travel and resupply assignments., preferred, , must possess:    demonstrated ability to effectively solve problems and or to demonstrate ability to deal with all levels in the business.', 'demonstrated success in managing a diverse team within an integrated organization, and demonstrated success in managing multiple, competing or competing projects.', 'demonstrated success in communication and teamwork skills, including presentation and critical thinking skills to influence business leaders to pursue and deliver on key strategic projects.', 'excellent planning skills in time-domain and project management.', ', knowledge, skills and Responsibilities include, but are not limited to:   business unit, time management and organizational competency   analytics, including multiple data sources and large datasets.', 'analytics, including relational data base modelling, dimensional analysis and segmentation tracing   data modelling, including segmentation tracing   data querying, including data loading, data validation, data transformation, data visualization, data manipulation, data queries, data mapping   data quality management and data integration   data link-up, including data quality mapping   data analytics, including data cleansing, data manipulation, data visualization, data visualization, data profiling   data health management (preferably health statistics, prevalence and related medical conditions)   data planning and project management   data collection, integration and dissemination   data profiling and visualizations   data querying including querying data lakes and physical databases in order to identify potential conflicts and inconsistencies   data lifecycle including the data lifecycle from source acquisition and implementation to test phase-to-test and quality-control processes   data requirements management including project requirements data quality management including project quality management   data quality integration, if applicable,     desired skills & abilities:,    experience   b.s.', 'or equivalent work experience.', '3 year relevant project management experience   demonstrated ability to take on conflicting and pressing business and operational priorities as assigned   demonstrated ability to understand and apply customer and market needs to enable innovative solutions to improve customer experiences   demonstrated ability to work on both fixed and variable project work streams   strong interpersonal, written communication and presentation skills to lead and influence work across functional areas and leadership   strong project capacity and organizational skills to support and lead multiple projects in an interdependent environment   experience and interest in the automotive and or service sectors – automotive service, engineering service, operations service.', 'preferred – advanced calculus, data analytics, model selection, classification, modeling, statistics, predictive modelling, and statistical modelling   preferred – advanced analytical and statistical skills including problem solving, predictive modelling, service analytics and analytical approaches.', 'required – experience working in one or more analytical and statistical teams   preferred – demonstrated ability to work with a diverse set of stakeholders   work knowledge and experience in the use of analytical tools – for example, hadoop, spark, hive, scala, sql, tableau, shiny and python   ]']\n","739\tsenior data scientist senior\ttensorflow hadoop r sql tableau spark nosql python html5\t['we are looking for a candidate who has 3+ years of experience in a data science role, along with 4+ years of relevant experience within an ecommerce analytics role.', 'ideally you have experience on a variety of topics, preferably – but not limited to – statistical analysis, modeling, scripting, statistical testing, machine learning.,    about our role – lead quantitative analysis of large structured and unstructured datasets   build statistical and predictive models using general purpose machine learning techniques such as k-means, linear mixed-effects regression, bayesian inference, neural networks, hierarchical linear modeling, and statistical modeling   communicate your results in a clear, precise, and actionable manner,    the experience and technical expertise our team brings to the table,    experience is a definite plus,    key responsibilities   , help shape the future of our platform by delivering “big data” information to drive action,    what we're doing   , we're building a massive data driven analytics platform that aims to revolutionize consumer experiences and drive better decisions.', 'you will be a consultant to the marketing, sales and pricing team and work on global projects to help drive improvements in both customer and product experiences.', 'if you have extensive experience working on big data, and want to take on some heavy lifting and help shape the data, then we'd love to hear from you.', ', you'll create and deliver descriptive, textural and graph models using one-off data.', 'and\n","\n","[810 | 1063.13] loss=1.26 avg=2.07\n","[820 | 1075.56] loss=1.73 avg=2.06\n","[830 | 1087.99] loss=1.21 avg=2.04\n","[840 | 1100.43] loss=1.71 avg=2.04\n","[850 | 1112.86] loss=1.42 avg=2.03\n","[860 | 1125.30] loss=1.52 avg=2.02\n","[870 | 1137.74] loss=1.70 avg=2.01\n","[880 | 1150.18] loss=1.18 avg=2.00\n","[890 | 1162.62] loss=0.69 avg=1.98\n","[900 | 1175.08] loss=1.29 avg=1.97\n","Saving checkpoint/run2/model-900\n","[910 | 1189.72] loss=1.13 avg=1.95\n","[920 | 1202.15] loss=0.93 avg=1.93\n","[930 | 1214.59] loss=1.38 avg=1.93\n","[940 | 1227.02] loss=1.19 avg=1.91\n","[950 | 1239.44] loss=1.54 avg=1.91\n","[960 | 1251.87] loss=1.54 avg=1.90\n","[970 | 1264.30] loss=1.03 avg=1.89\n","[980 | 1276.75] loss=1.39 avg=1.88\n","[990 | 1289.17] loss=0.87 avg=1.86\n","[1000 | 1301.60] loss=1.20 avg=1.85\n","Saving checkpoint/run2/model-1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VHdTL8NDbAh3"},"source":["gpt2.copy_checkpoint_to_gdrive(run_name='run2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQJgV_b4bmzd"},"source":["You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."]},{"cell_type":"markdown","metadata":{"id":"pel-uBULXO2L"},"source":["## Load a Trained Model Checkpoint\n"]},{"cell_type":"code","metadata":{"id":"DCcx5u7sbPTD"},"source":["gpt2.copy_checkpoint_from_gdrive(run_name='run1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTa6zf3e_9gV"},"source":[""]},{"cell_type":"code","metadata":{"id":"-fxL77nvAMAX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616825980140,"user_tz":-330,"elapsed":13203,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"6213547c-4bba-4b17-e75d-02e37ecba608"},"source":["sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, run_name='run1')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading checkpoint checkpoint/run1/model-1000\n","INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ClJwpF_ACONp"},"source":["## Generate Text From The Trained Model\n","\n","After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."]},{"cell_type":"code","metadata":{"id":"4RNY6RBI9LmL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616767488142,"user_tz":-330,"elapsed":12796,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"b09f42af-8bb6-455f-f7d5-2f78f8108ce5"},"source":["gpt2.generate(sess, run_name='run1')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["advertising\n","\n","\"they're just getting started!\" - one of the founders of aws, you might have heard, has just turned 50 (not quite yet eligible for a government sponsorship program yet).',#\"bashononon\" says the title of this post, but google will clearly identify the title on its support forums and in the google docs once this role has been over).',#\"#bashononon\" is a good day to be a software engineer in aws, aws cloud, or any other big data project, especially one that involves data at scale     some background in hadoop and some familiarity with hive       aws is a fantastic company to work with      #bashononon\" says the title of this post, but google will clearly identify the title on its support forums and in the google docs once this role has been over).]']\"\n","5126\tseniormap data engineer\thive machine learning hadoop r sql data warehouse s3 spark kubernetes git nosql python aws\t\"['this role reports to the vp of hardware engineering and builds the infrastructure for continuous integration (cis) of data sets with the target of 30%+ of the time.,    this role reports to the director of hardware engineering and builds the infrastructure for continuous integration (cis) of data sets with the target of 30%+ of the time.,    this role reports to the sdr of hardware engineering and builds the infrastructure for continuous integration of data sets with the target of 30%+ of the time.,    this role reports to the director of software engineering and builds the infrastructure for continuous integration of data sets with the target of 30% of the time.,    this role reports to the director of systems engineering and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it infrastructure and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it infrastructure and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target of 10% of the time.,    this role reports to director of it and builds the infrastructure for continuous integration of data sets with the target\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oF4-PqF0Fl7R"},"source":["If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n","\n","You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n","\n","You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n","\n","Other optional-but-helpful parameters for `gpt2.generate` and friends:\n","\n","*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n","* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n","* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n","* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n","* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n","*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."]},{"cell_type":"code","metadata":{"id":"8DKMc0fiej4N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616826163613,"user_tz":-330,"elapsed":12164,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"3d28774f-cba2-4c96-d156-e5401f5866a2"},"source":["gpt2.generate(sess,\n","              prefix=\"data science sap sql\",\n","              length=150,\n","              temperature=0.7,\n","              nsamples=5\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data science sap sql json phd or higher in a quantitative or analytical field   3 years of experience with statistical or machine learning modeling skills (e.g., regression, support vector machine, clustering, etc.)', '4 years of experience with using tableau, or experience with a similar technology (e.g., r, sas, or matlab,    experience with object-oriented programming, query optimization, data integration (sql, hive, sql server),    experience with bi tools (e.g.', 'tableau),    experience with statistical packages (e.g., r, sas, or matlab,    experience with object-oriented programming, query optimization, data integration (sql,\n","====================\n","data science sap sql tableau\t\"['you will be a part of a new initiative called machine learning at elecive, and will be working on machine learning with a wide variety of data sources from various sources, including social networks, email, search engine results, news, and internal external data.', 'as the principal data scientist, you will report to the chief data scientist, and you will work on projects that bring together a variety of disciplines from machine learning, data mining, statistical analysis, and more.', 'this position requires up to 50%, although this position may be filled at any time.', ', the ideal candidate will be a self-starter with the ability to make a deep technical impact, and a pattern recognition and decision tree approach to solving\n","====================\n","data science sap sql html5 tableau java spss matlab python\t['[position description, data science role, required skills, bachelor’s degree in a quantitative analytical field such as applied math, statistics, physics, or computer science or equivalent experience minimum 8 years of experience in a quantitative analytical role in a team environment and in a commercial setting minimum 8 years of experience in a quantitative analytical role in a commercial setting minimum 8 years of experience in a quantitative analytical role in a commercial setting minimum 8 years of experience in a quantitative analytical role in a commercial setting reporting and analysis, including creation and implementation of dashboards and analysis reports, required skills, knowledge of core rdbms processes such as transformation, normalization, validation, integration, anomaly detection, and time\n","====================\n","data science sap sql html5 tableau python\t['[the team: the data science team is a newly formed applied research team within s&p global ratings that will be responsible for building and executing a bold vision around using machine learning, natural language processing, data science, knowledge engineering, and human computer interfaces for augmenting various business processes.,    responsibilities:   , build machine learning models with ai to solve various business problems including the following:\n","\n","- natural language processing\n","\n","- data science algorithms\n","\n","- deep learning frameworks such as tensorflow, pytorch or similar\n","\n","- statistical analysis of data\n","\n","- implement models using sis such as r, matlab, ipython, etc.', 'to build the\n","====================\n","data science sap sql big data data analytics\t\"['[position description, implementation details, performance measurements, and analytical methods,    specific responsibilities,    perform data mining and develop insights into patterns and trends in a high throughput, highly-customized data science environment,    develop data science models using r, python, or sas and ingest data from a variety of sources (sql, source control systems (ssheres), data visualization tools, and other tools to produce actionable insights   serve as a subject matter expert in data science and analysis and provide recommendations on data quality for all data science projects   assist in designing and developing data models that combine statistical analysis with predictive modeling and statistical techniques to build products   evaluate the data models\n","====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQCVfyTBWofW","executionInfo":{"status":"ok","timestamp":1616827152376,"user_tz":-330,"elapsed":10497,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"7db0f6ae-a3c4-45e9-b3a9-02e9dfce97a1"},"source":["gpt2.generate(sess,\n","              prefix=\"Associate data scientist AI Machine Learning Network R SAS C/C++ Java SPSS Data SciencePython\",\n","              length=150,\n","              temperature=0.7,\n","              nsamples=5\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Associate data scientist AI Machine Learning Network R SAS C/C++ Java SPSS Data SciencePython sql .net java matlab data warehouse oop docker ci software development mvc\t\"[\"\"lead the development of predictive analytics solutions, including machine learning, regression, and statistical methods by creating and maintaining data products and reporting.\"\", 'you will provide insights from your data and your reporting into the business where the reporting is coming from., you will lead the use of statistical analysis, machine learning and predictive modeling to support our decision support initiatives across the economy.', \"\"you will be responsible for identifying and analyzing trends, identifying areas for future projects, and providing insights from these trends., you will be responsible for delivering insights from our data so that we can make more informed investment decisions., you will partner with clients to identify areas of opportunity, recommend\n","====================\n","Associate data scientist AI Machine Learning Network R SAS C/C++ Java SPSS Data SciencePython� matlab python\t\"['this role will also contribute to the development and validation of advanced analytics tools, such as predictive models, machine learning algorithms, and data visualization techniques.,    this role requires strong communication, analytical, and problem solving skills.', 'you will be required to be a team player, able to quickly comprehend complex technical requirements, analyze data, and make intelligent choices.', 'you will also be expected to be a strong learner, who is eager to explore new analytic techniques and develop business value.,    we offer: ,    competitive salary and comprehensive benefits including medical, dental, vision, and 401(k) matching   tuition reimbursement   pre-tax full time federal government employee assistance\n","====================\n","Associate data scientist AI Machine Learning Network R SAS C/C++ Java SPSS Data SciencePython java c#\t['[   uncover new sources of insight into complex business processes through intuitive data-driven storytelling tools that transform data into actionable insights   , role description,    we are looking for a highly motivated individual with 2-4 years’ experience in an analytical data scientist role, with a strong quantitative background, to be considered.', 'they should also have experience using the following software tools:   , quantitative analysis tools (e.g., cuda, matlab),    data aggregation tools (e.g., r, python),    data processing frameworks (e.g., spark),    data wrangling tools (e.g., rdcpu),   \n","====================\n","Associate data scientist AI Machine Learning Network R SAS C/C++ Java SPSS Data SciencePython\t\"['[   apply your expertise in quantitative analysis, data mining, and the presentation of data to discover, uncover, and improve our customers’ business needs through data driven decision making.,    in this role you will:,    design and analyze large, complex datasets with structured and unstructured data, and conduct exploratory and decision-making analyses in order to generate valuable business recommendations and actionable recommendations from data to be included in decision-making.', 'consult with product and engineering teams to identify requirements and issues, develop and recommend improvements to data collection processes and tools.', 'build and document algorithms that address various business problems using multiple data sources and or tools,    you have extensive experience in\n","====================\n","Associate data scientist AI Machine Learning Network R SAS C/C++ Java SPSS Data SciencePython\t\"['the role is to identify, analyze, and interpret quantitative data from varying sources into a form understandable and easily understood by stakeholders.,    this is a hands-on opportunity to use your experience in quantitative analysis to evaluate the impact of decision on downstream business outcomes, and therefore will look to hire a data scientist with a deep knowledge of the data.,    in particular you will want to know the following:,    what you’ll learn about upstream business from your data analysis:,    benefits we offer:,    competitive salary and bonus   401k with match   annual bonus matching   health insurance company match   on-site fitness center matching   tuition reimbursement   employee discount\n","====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fKN79y5p3htc","executionInfo":{"status":"ok","timestamp":1616827622631,"user_tz":-330,"elapsed":7892,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"ac4d9040-8beb-4fca-a608-178cd67de936"},"source":["output = gpt2.generate(sess,\n","              prefix=\"data science AI Quantitative Analysis Data Mining Machine Learning Analysis Skills CSS\",\n","              length=150,\n","              include_prefix=False,\n","              truncate=False,\n","              temperature=0.7,\n","              nsamples=2\n","              # ,\n","              # batch_size=5\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data science AI Quantitative Analysis Data Mining Machine Learning Analysis Skills CSS (manual search)\n","3.1 leadership skills, experience in leadership, strategic thinking, and problem-solving.', 'experience in managing and leading project teams and managing projects across three or more analytical disciplines, including research and development of analytical approaches and data models.', 'experience in managing and leading project teams and managing projects across three or more analytical disciplines, including research and development of analytical approaches and data models.', 'experience in managing and leading project teams and managing projects across three or more analytical disciplines, including research and development of analytical approaches and data models.', 'experience in managing and leading project teams and managing projects across three or more analytical disciplines, including research and development of analytical approaches and data models.',\n","====================\n","data science AI Quantitative Analysis Data Mining Machine Learning Analysis Skills CSS (physics, actuarial or technical data),    education required:,    master's degree in a “stem” major (science, technology, engineering, mathematics) or equivalent combination of education and experience preferred.', 'phd in a “stem” major (science, technology, engineering, mathematics) or equivalent combination of education and experience.', 'minimum of four years of professional experience in a similar position.', 'significant experience in applying data science to real world problems in a highly creative and innovative way,    preferred skills:,    programming experience in r or python.', 'experience with statistical analysis and predictive modeling in a high-level language such as python or java.',\n","====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"SokHZm9ZuhzE","executionInfo":{"status":"error","timestamp":1616681833009,"user_tz":-330,"elapsed":975,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"1cd322e7-d36c-4c16-f90f-7255a3e69e79"},"source":["for i, output in enumerate(output):\n","    print(i)\n","    print(output)\n","    # text = tokenizer.decode(output, skip_special_tokens=True)\n","    # a = len(\"data science sap sql java\")\n","    # print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-a4d32cbbf047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# text = tokenizer.decode(output, skip_special_tokens=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# a = len(\"data science sap sql java\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]}]},{"cell_type":"markdown","metadata":{"id":"zjjEN2Tafhl2"},"source":["For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n","\n","You can rerun the cells as many times as you want for even more generated texts!"]},{"cell_type":"code","metadata":{"id":"Fa6p6arifSL0"},"source":["gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=500,\n","                      temperature=0.7,\n","                      nsamples=100,\n","                      batch_size=20\n","                      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-LRex8lfv1g"},"source":["# may have to run twice to get file to download\n","files.download(gen_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQAN3M6RT7Kj"},"source":["## Generate Text From The Pretrained Model\n","\n","If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n","\n","This is currently the only way to generate text from the 774M or 1558M models with this notebook."]},{"cell_type":"code","metadata":{"id":"hsUd_jHgUZnD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616679791292,"user_tz":-330,"elapsed":1953,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"f33b6703-2c15-4f99-fe2c-7633748696ed"},"source":["# model_name = \"774M\"\n","model_name = \"run1\"\n","\n","gpt2.download_gpt2(model_name=model_name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fetching checkpoint: 1.05Mit [00:00, 334Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 290Mit/s]                                                    \n","Fetching hparams.json: 1.05Mit [00:00, 928Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 1.05Mit [00:00, 454Mit/s]                                  \n","Fetching model.ckpt.index: 1.05Mit [00:00, 404Mit/s]                                                \n","Fetching model.ckpt.meta: 1.05Mit [00:00, 413Mit/s]                                                 \n","Fetching vocab.bpe: 1.05Mit [00:00, 392Mit/s]                                                       \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BAe4NpKNUj2C"},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.load_gpt2(sess, model_name=model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xInIZKaU104","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1616680391566,"user_tz":-330,"elapsed":915,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"0c015db5-5747-47da-d50d-b2b91fedc3d5"},"source":["output = gpt2.generate(sess,\n","              model_name=model_name,\n","              prefix=\"data science sap sql java\",\n","              length=100,\n","              temperature=0.7,\n","              top_p=0.9,\n","              nsamples=5\n","              # ,\n","              # batch_size=5\n","              )"],"execution_count":null,"outputs":[{"output_type":"error","ename":"JSONDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f1792e989423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m               \u001b[0;31m# ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0;31m# batch_size=5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, sample_dir, return_as_list, truncate, destination_path, sample_delim, prefix, seed, nsamples, batch_size, length, temperature, top_k, top_p, include_prefix)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_hparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hparams.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/encoder.py\u001b[0m in \u001b[0;36mget_encoder\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoder.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab.bpe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mbpe_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\ufeff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             raise JSONDecodeError(\"Unexpected UTF-8 BOM (decode using utf-8-sig)\",\n\u001b[0;32m--> 338\u001b[0;31m                                   s, 0)\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)"]}]},{"cell_type":"markdown","metadata":{"id":"ig-KVgkCDCKD"},"source":["# Etcetera\n","\n","If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"]},{"cell_type":"code","metadata":{"id":"rIHiVP53FnsX"},"source":["!kill -9 -1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmTXWNUygS5E"},"source":["# LICENSE\n","\n","MIT License\n","\n","Copyright (c) 2019 Max Woolf\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE."]}]}