{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Task3.3_GPT2Model_AboutCompany.ipynb","provenance":[{"file_id":"https://github.com/MuchenZhang/cooking-AI/blob/master/Copy_of_Train_a_GPT_2_Text_Generating_Model_w_GPU.ipynb","timestamp":1616653277598}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"KBkpRgBCBS2_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618121133743,"user_tz":-330,"elapsed":3457,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"f40b10c0-b90e-4e60-b81d-8f0568de5b2a"},"source":["%tensorflow_version 1.x\n","!pip install -q gpt-2-simple\n","import gpt_2_simple as gpt2\n","from datetime import datetime\n","from google.colab import files\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sUmTooTW3osf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618121133745,"user_tz":-330,"elapsed":3455,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"19120ccc-a828-4d5b-fc95-55927479fe9f"},"source":["!nvidia-smi"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Sun Apr 11 06:05:33 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8wSlgXoDPCR","executionInfo":{"status":"ok","timestamp":1618121234076,"user_tz":-330,"elapsed":103779,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"51e33ac5-941a-41ec-f065-e5848813e998"},"source":["gpt2.download_gpt2(model_name=\"124M\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Fetching checkpoint: 1.05Mit [00:00, 510Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 2.73Mit/s]\n","Fetching hparams.json: 1.05Mit [00:00, 492Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 498Mit [01:37, 5.12Mit/s]\n","Fetching model.ckpt.index: 1.05Mit [00:00, 449Mit/s]                                                \n","Fetching model.ckpt.meta: 1.05Mit [00:00, 2.91Mit/s]\n","Fetching vocab.bpe: 1.05Mit [00:00, 2.96Mit/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puq4iC6vUAHc","executionInfo":{"status":"ok","timestamp":1618121312165,"user_tz":-330,"elapsed":1054,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"35f7bf44-6ad1-49c2-d5cd-5b9ca1a5f442"},"source":["gpt2.mount_gdrive()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6OFnPCLADfll","executionInfo":{"status":"ok","timestamp":1618121287354,"user_tz":-330,"elapsed":1044,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["file_name = \"whyus_dataset.txt\""],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"Zfmr0biwDJ43","executionInfo":{"status":"ok","timestamp":1618121234079,"user_tz":-330,"elapsed":103752,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"d5c7503a-a598-46e4-ec6d-dd1f8b8e4785"},"source":["\n","netflix_data = pd.read_excel('/content/drive/My Drive/JobDescriptionPrediction/Resources/Data/processed/whyus_dataset.xlsx')\n","netflix_data[['JobID','Company','Location','Output']].head(n=2)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>JobID</th>\n","      <th>Company</th>\n","      <th>Location</th>\n","      <th>Output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Express Scripts</td>\n","      <td>MO</td>\n","      <td>['and most importantly that you have unquestio...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13</td>\n","      <td>Rice University</td>\n","      <td>TX</td>\n","      <td>[\"[mid data scientist, our client in the midto...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   JobID  ...                                             Output\n","0      1  ...  ['and most importantly that you have unquestio...\n","1     13  ...  [\"[mid data scientist, our client in the midto...\n","\n","[2 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"d7x1CBGYrJz-","executionInfo":{"status":"ok","timestamp":1618121234080,"user_tz":-330,"elapsed":103743,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["TRAIN_SIZE      = 0.8\n","def split_data(netflix_data, S=TRAIN_SIZE):\n","    print(TRAIN_SIZE)\n","\n","    # Split into training and validation sets    \n","    train_size = int(S * len(netflix_data))\n","\n","\n","    train_data = netflix_data[:train_size]\n","    val_data = netflix_data[train_size:]\n","\n","    return train_data, val_data"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"yZvmdayprKjs","executionInfo":{"status":"ok","timestamp":1618121234081,"user_tz":-330,"elapsed":103730,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"62ad9ebb-17d7-4bb8-8158-416e883677f7"},"source":["train_data, val_data = split_data(netflix_data, TRAIN_SIZE)\n","\n","f'There are {len(train_data) :,} samples for training, and {len(val_data) :,} samples for validation testing'"],"execution_count":22,"outputs":[{"output_type":"stream","text":["0.8\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'There are 236 samples for training, and 60 samples for validation testing'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"pDrYgT0GRt_n","executionInfo":{"status":"ok","timestamp":1618121234082,"user_tz":-330,"elapsed":103720,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["train_data.to_csv('whyus_dataset.txt', header=True, index=False, sep='\\t', mode='w')"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z6okFD8VKtS"},"source":["# gpt2.copy_file_from_gdrive(\"/content/drive/My Drive/JobDescriptionPrediction/Resources/Data/processed/whyus_dataset.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKQs9H8IN1X-","executionInfo":{"status":"aborted","timestamp":1618121234723,"user_tz":-330,"elapsed":104342,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdpZQXknFNY3"},"source":["## Finetune GPT-2\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeXshJM-Cuaf","executionInfo":{"status":"ok","timestamp":1617287157512,"user_tz":-330,"elapsed":2475028,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"36a7f5c8-42a2-4274-d6a4-8ece24cb9073"},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.finetune(sess,\n","              dataset=file_name,\n","              model_name='124M',\n","              steps=1000,\n","              restore_from='fresh',\n","              run_name='run3',\n","              print_every=10,\n","              sample_every=200,\n","              save_every=100\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Loading checkpoint models/124M/model.ckpt\n","INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loading dataset...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["dataset has 132923 tokens\n","Training...\n","[10 | 28.31] loss=3.41 avg=3.41\n","[20 | 49.96] loss=2.96 avg=3.19\n","[30 | 72.11] loss=2.70 avg=3.02\n","[40 | 94.86] loss=2.56 avg=2.91\n","[50 | 118.32] loss=2.49 avg=2.82\n","[60 | 142.71] loss=2.37 avg=2.74\n","[70 | 166.40] loss=2.10 avg=2.65\n","[80 | 189.87] loss=2.54 avg=2.63\n","[90 | 213.62] loss=2.37 avg=2.60\n","[100 | 237.47] loss=2.22 avg=2.56\n","Saving checkpoint/run3/model-100\n","[110 | 263.91] loss=1.57 avg=2.47\n","[120 | 287.71] loss=1.43 avg=2.38\n","[130 | 311.49] loss=1.56 avg=2.31\n","[140 | 335.22] loss=1.47 avg=2.25\n","[150 | 358.86] loss=1.45 avg=2.19\n","[160 | 382.54] loss=1.41 avg=2.14\n","[170 | 406.35] loss=1.10 avg=2.07\n","[180 | 430.13] loss=1.15 avg=2.01\n","[190 | 453.89] loss=0.62 avg=1.93\n","[200 | 477.63] loss=0.98 avg=1.88\n","Saving checkpoint/run3/model-200\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","======== SAMPLE 1 ========\n"," businesses we see an exponential volume in each year.,     we at nimble rely on a solid self-service model that ensures everyone has a level playing field within our firm.', 'see our methodology and learn more at nimble.co.uk or follow us on slack @nimblepowered.', 'store to expand opportunities with “industry” eager partners!', 'build stronger, better service smarter.', 'implement best practices knowledge-driven and iterative approach   , the intent of nimble is to help you grow and advance your career.', 'this includes using your nps skills to help nimble, nps gm, to design tools that will empower engineers and business stakeholders to better understand and influence business decisions.', 'build upon our proprietary platform, develop best in class applications, provide financial and fraud support, and help nimble identify and detect cyber-attacks.', 'utilize knowledge of nimble frameworks, including statistics, advanced mathematics and artificial intelligence, to create tools for engineers and business stakeholders to better understand and influence business decisions.', 'develop and support code for application layer technologies, such as sas, nlp and stealth stack, for example.', 'develop and support hardware-accelerated image processing, sound generation and speech generation for neural networks, social networking and content.', 'utilize knowledge of nimble frameworks, including statistics, advanced mathematics and artificial intelligence, to create tools for engineers and business stakeholders to better understand and influence business decisions.', 'collaborate with other disciplines, such as mathematics, algorithms and machine learning, to create highly scalable and scalable algorithms, analytics, machine learning, nlp and stealth stack.', ',    for more information, visit our careers page ( https:  www.nimble.com careers section )', ',   nimble is an equal employment opportunity employer minority female sexual orientation gender identity individual with disability protected veteran.', 'we welcome all persons to apply.,    how to apply:,    for employment with us, please review this guide .', \"\"apply now, now, or in future sessions , topics:, advanced analytics, machine learning, cyber-security, gender identity, disability, available here: careers.,    digital marketing,    healthcare,   research,   preferred:,    fulfillment, automation, machine learning, social responsibility,    careers,    consulting,    healthcare,    development,    comfort living,    research and application,    benefits,    accommodation and start enjoying your new new new new adventure , job summary , the role tracks applicants progress and provides flexibility to resign their posts if need be .\"\", 'the qualifications and responsibilities are related to the candidate's location, education, military experience and national origin, and are in addition to his or her environmental supply contract position.', 'if the candidate is unable to work because of a health condition, preventable mental health or medical condition, disability, or disability, he or she should seek employment with us or another third party vendor.,    ticket buy,    what you get out of here, candidates!', 'a career with us is an opportunity for us to establish what it means to be an employee at ticket buy, and we're constantly refining our tool kits with new features to see what works and, more importantly, testing new features.', 'while we do this by identifying which tools are most useful to users, we also provide tailored help and support packages that meet specific needs and organizational goals.', 'applicants who use their power, or point of view, to the best interest of their clients are strongly encouraged to pursue this role with us. ]']\n","1024\tGCC Technologies, LLC\tDC\t11471 12 equal employment opportunity for all associates qualified for benefits, on site, after processing, to return to work within 30 days of applying.\"\", 'if this is your first time applying, you are welcome to return to work within 30 days.', 'if not, please send a cover letter, e-mail, and any queries, which you may forward to us by mail, November 22, 2018, with your subject line.\"\"\", 'requests for employee assistance during the processing of this application may be directed to administrative, human resources, or technical support.', 'in addition, we strongly encourage applicants to bring their own personal and technical skills to the research and development out of this application.', 'requests for reasonable accommodation under thelicients list should be directed to administrators, data translators and architects.', 'upside down problem solving skills will be trained to prepare employees for any unexpected challenges or the unexpected.', 'hands-on project management skills, as well as project-driven approach including participating in drafting of processes and recommendations, review of federal programs that provide public safety and other benefits to members of law enforcement and law enforcement communities, as well as participating in drafting of\n","\n","[210 | 514.99] loss=1.13 avg=1.84\n","[220 | 538.88] loss=0.80 avg=1.79\n","[230 | 562.82] loss=0.50 avg=1.73\n","[240 | 586.73] loss=0.62 avg=1.68\n","[250 | 610.51] loss=0.54 avg=1.63\n","[260 | 634.28] loss=0.51 avg=1.58\n","[270 | 658.07] loss=0.52 avg=1.53\n","[280 | 681.86] loss=0.57 avg=1.49\n","[290 | 705.64] loss=0.37 avg=1.45\n","[300 | 729.37] loss=0.27 avg=1.40\n","Saving checkpoint/run3/model-300\n","[310 | 755.57] loss=0.29 avg=1.36\n","[320 | 779.56] loss=0.44 avg=1.33\n","[330 | 803.36] loss=0.59 avg=1.30\n","[340 | 827.13] loss=0.20 avg=1.26\n","[350 | 850.99] loss=0.31 avg=1.23\n","[360 | 874.87] loss=0.27 avg=1.20\n","[370 | 898.77] loss=0.33 avg=1.17\n","[380 | 922.68] loss=0.16 avg=1.14\n","[390 | 946.50] loss=0.16 avg=1.11\n","[400 | 970.28] loss=0.23 avg=1.08\n","Saving checkpoint/run3/model-400\n","======== SAMPLE 1 ========\n"," sap ai systems hadoop data mining and deep learning hadoop architectures hadoop seq chocolat sas sap jupiter, but this is a story of many departments and projects), how you are an individual capable of doing different things a company culture that is more like a professional culture than a lab environment , a career with us means protecting the intellectual property of our employees and clients, limiting disruption and improving business results.', 'our model is a partnership: we work together and do it other people's work!, why join,    at nextera, you will be working in a modern, technology-based company with technologists, who are building the future of big data and computing into a center of innovation., to learn more about nextera and its different departments, check out our up front and long term overview., our mission at nextera is to derive important insights from diverse sources, using machine learning, advanced analytics, computer vision, and other relevant technologies, according to the principal author(s) of the report, \"\"what we learn, our role models , ca- mit partners with engineering and data analysis to develop, maintain, and improve transacting solutions utilizing machine learning, advanced algorithms, artificial intelligence, and business acumen's insights to build, new sources of machine learning data reveal patterns,iances, correlations, and anomalies in data and provide experimental study plans   ca- mit researchers are responsible for providing experimental study plans and or studies to serve as a base for development of experimental manipulations that may or may not become operational ca- mit is an equal opportunity employer.', 'we are an equal opportunity affirmative action employer and are proud to be an equal opportunity affirmative action employer inclusive of qualified applicants without regard to race, color, creed, religion, ancestry, national origin, sex, sexual orientation, gender identity, age, disability, veteran status, protected veteran status or any other characteristic protected by law.]']\"\n","738\tNavigator Solutions\tNY\t['the team is currently led by an experienced set of consumer technology veterans with deep expertise in logic and machine learning.', 'the team also includes innovators, software engineers, product managers, product managers still design products and software but are using machine learning to new levels.,    what you will do   ideation and research will be your strong focus.', 'you will be designing the data specification   ai   algorithms for analyzing the specified data   modeling programs for optimizing various aspects of the engineered environment   can be a full-time position, but we do have half-time to pay the interns   who make up half of the team are freelancers   we provide reasonable vacation time offering you the opportunity to work in one of our offices for less than $10 an hour   we have a program for your mobile phone   take advantage of today\\'s opportunity to get a 3 hour day   we will consider ~ $40 - $70 better than in your first year   education must be accredited by an accrediting organization   education must meet the following performance standards   there must be 1-2% variance in the test results compared with what you achieved in your first year   the cost of attendance will be waived on your first year\\'s income, up to and including your waiver application   there must be 1-2% variance in the hours worked, on average, by employee corporation employee   education must meet the following performance standards:   XXL H1N1 Visa Signature Health Plan - 120 hours working week   One course meal plan with every shift   $10 off your first paycheck   401k match   help with any extra costs   summer intern training   we offer competitive health and dental insurance to all, regardless of religion, race, religion preference, national origin, ancestry, citizenship, marital status, or medical condition.', 'at the end of every working day, we ask people to get a break from work to have a family member in the hospital.', 'at the end of the week, we will move the family   to a new office location quickens transition to a new office location time off from work greatly increases the number of visits, time off from work, but not hours, so be sure to tell your family you are there.', \"\"we are an equal opportunity affirmative action employer and all qualified applicants will receive consideration for employment without regard to race or color, colorism, religion, gender identity and expression, sexual orientation, gender identity, national origin, disability, age, physical, mental, sensory, or sensory disability, protected veteran status or any other characteristic protected by law.,    what you will hear from medical school about,    u.s. men and sizes 8-11 years of male to female\"\"]\"\n","739\tHealthes\tCO\t['the team uses a data warehouse across tensorflow that provides scalable and ready-to-use solutions to customers.', 'we do this\n","\n","[410 | 1006.40] loss=0.20 avg=1.06\n","[420 | 1030.26] loss=0.13 avg=1.03\n","[430 | 1053.99] loss=0.16 avg=1.01\n","[440 | 1077.93] loss=0.15 avg=0.98\n","[450 | 1101.80] loss=0.07 avg=0.96\n","[460 | 1125.54] loss=0.14 avg=0.93\n","[470 | 1149.35] loss=0.09 avg=0.91\n","[480 | 1173.28] loss=0.12 avg=0.89\n","[490 | 1197.08] loss=0.11 avg=0.87\n","[500 | 1220.80] loss=0.08 avg=0.85\n","Saving checkpoint/run3/model-500\n","[510 | 1246.82] loss=0.13 avg=0.83\n","[520 | 1270.74] loss=0.07 avg=0.81\n","[530 | 1294.56] loss=0.14 avg=0.80\n","[540 | 1318.29] loss=0.07 avg=0.78\n","[550 | 1342.04] loss=0.07 avg=0.76\n","[560 | 1365.86] loss=0.08 avg=0.75\n","[570 | 1389.79] loss=0.10 avg=0.73\n","[580 | 1413.66] loss=0.09 avg=0.72\n","[590 | 1437.48] loss=0.07 avg=0.70\n","[600 | 1461.30] loss=0.07 avg=0.69\n","Saving checkpoint/run3/model-600\n","======== SAMPLE 1 ========\n",", and experience is required; we do not accept unwarranted advances nor are we responsible for any fees associated with your use of our platform.]']\n","2287\tKPMG LLP\tIL\t['that’s why we employ the most creative, passionate people in the industry.,    the challenge ahead,    the ea digital platform data & ai group is responsible for providing unified artificial intelligence (ai) resources across all franchises within electronic arts.', 'our group develops state-of-the-art machine learning, ai, and data-driven solutions to game team problems, as well as hands-on experience in leveraging big datasets to create insightful, practical solutions.', 'while we are part of a rapid change in the way we do business that could transform the way gamers enjoy video games, we are a very new breed of startup.', 'the kind that helps make a difference in our gamers lives.,    the role,    the team's role,    the ea digital platform data & ai group operates autonomously, working with only the most passionate of stakeholders, ensuring the game is dynamic, engaging, and consistently updated with new insights and insights.', 'the team also provides guidance and expertise to other teams, providing the necessary analytical rigor and awareness to ensure data quality, data integrity, and helps facilitate better understanding of the various aspects of the game we support (ai, physics, gameplay design, engineering, story).,    requirements and basic requirements,     accomplished scientist - with a background in statistics, data mining, algorithmic complexity, or artificial intelligence that results in a focused, sophisticated, and precise analysis   strong scientist with a background in machine learning (eg linear modeling, random forests, ensemble functions, ensemble functions, etc), implementing algorithms and machine learning methods that support game play and emotional understanding of the player decision making processes   advanced knowledge of sql, hive, etl   ability to adapt to new environments and get better at your craft   extensive knowledge in data modeling, sql, hive, etl, relational databases, application programming Interface (pc), web application   python, r, c++, html, javascript   understanding of science and technology, of varying complexity, education and level of training   strong clinical understanding of the gaming industry, particularly PC Gamer,    experience with gamingannex and or our survey partners demonstrates a good understanding of the subject matter of data scientists and data engineers, and is coupled with a sensitivity level of lower than 3%, to enable development of products that meet the needs of the organizations needs   for this purpose, the incumbent data scientist(s) will be responsible for providing the required training and credentials, and working with colleagues to develop products that meet the needs of the organization   experience in building predictive analytics solutions using available resources in the analytics industry is required   experience in working with raw data and extracting and processing the resulting data   proficiency in using mass spectrometry, on the fly, and query   education: bachelors in statistics, mathematics, operations research, computer science, engineering or one (one or more of the following): applied physics, physics, computer science, engineering, physics   minimum requirements:   , phd degree from a top tier physics, computer science, engineering, physics or related field   15+ years of experience performing analytics, algorithm development, analytical modeling or simulation   7+ years of experience using machine learning, supervised or unsupervised learning techniques, the candidate will demonstrate the ability to run complex analytics in a fast, distributed environment   strong communication and demonstrated ability to explain data science to managers, clients and subsidiary technical leadership   experience developing complex analytical and visualization projects, including presentations   advanced excel skills, including the ability to describe complex projects, and implement dashboard based solutions   strong knowledge in the use of quantitative algebra and statistics   strong knowledge in the technical rigor of calculating vs. optimizing a business response time and forecasting   familiar with statistical and numerical analysis techniques and able to work with senior analytics team   excellent organizational and material skills   good written and oral communication skills, plus the ability to interact professionally with executives, managers and technical leads   ability to thrive in a fast changing environment with changing priorities   environmental awareness and to mitigate risks ai   advocating job on social media and blogging about your experiences.]']\n","2281\tRocksBox\tCA\t['[   work directly with high-quality, data-driven, and insightful data teammates to help solve complex problems   design, test and implement predictive analytics and insights that support beverageCo customers   deliver innovative and original data-driven presentations to non-scientific audiences,    craft fresh, high-quality visual solutions for clients and partners to generate revenue   work with management team and provide solutions to improve current or previous client performance reviews,    design, test, implement, monitor, and evaluate current\n","\n","[610 | 1497.24] loss=0.06 avg=0.68\n","[620 | 1521.17] loss=0.06 avg=0.66\n","[630 | 1545.04] loss=0.07 avg=0.65\n","[640 | 1568.89] loss=0.05 avg=0.64\n","[650 | 1592.68] loss=0.06 avg=0.63\n","[660 | 1616.45] loss=0.06 avg=0.61\n","[670 | 1640.21] loss=0.07 avg=0.60\n","[680 | 1663.96] loss=0.07 avg=0.59\n","[690 | 1687.76] loss=0.07 avg=0.58\n","[700 | 1711.58] loss=0.05 avg=0.57\n","Saving checkpoint/run3/model-700\n","[710 | 1737.66] loss=0.04 avg=0.56\n","[720 | 1761.49] loss=0.05 avg=0.55\n","[730 | 1785.39] loss=0.06 avg=0.54\n","[740 | 1809.33] loss=0.05 avg=0.53\n","[750 | 1833.07] loss=0.06 avg=0.52\n","[760 | 1856.89] loss=0.06 avg=0.52\n","[770 | 1880.82] loss=0.05 avg=0.51\n","[780 | 1904.71] loss=0.06 avg=0.50\n","[790 | 1928.56] loss=0.08 avg=0.49\n","[800 | 1952.37] loss=0.07 avg=0.48\n","Saving checkpoint/run3/model-800\n","======== SAMPLE 1 ========\n"," using a broad range of analytics tools including bose, cpt, clustering engines, scikit-learn, pyspark, clustering packages:, ~ $10-20k total cao experience very similar skills and experience to seasoned professionals with a variety of business applications, including but not limited to: trading, credit risk analysis, alternative asset pricing, future of e-commerce price, marketplace, and subscription models   advanced cost methodologies like chi-squared, multivariate linear logistic regression, time-series analysis, and time series analysis using r or python (not available now) or any of the other tools specifically stated above,    additional qualifications,    detailed application instructions,    paid time off   tuition reimbursement   flexible spending   credit and debit cards   charity   carbon black and gre asa bhumi mjp business plan ahead   affiliate and paid time off offers   tuition reimbursement, copayments to qualified health plan members, and the like   monthly contribution to help pay for travel and childcare   up to an unlimited monthly salary   monthlycale with employer stock ownership company   affiliate and paid time off offer   monthly contribution to help pay for travel and childcare,    this job is uniquely positioned to provide comprehensive, employee-benefits-paid health and dental insurance to all, with the possibility of proactively getting medical, vision, and dental benefits before the event of an illness or accident,    this job is under federal law required credit union identification information and aholder of the eoe aa partners insurance plan.,    if you require assistance with a personal or work-related problem, please send an e-mail to askhr@health.gov with your request to career advice at problemoplexid@health.gov.', 'requests for employee assistance will be considered on a case-by-case basis.', 'hiv placements are not available in all cases.,    if you require assistance with a personal or business problem, please send an e-mail to askhr@health.gov.', 'you may include any technical information you may have access to in your request, but please do not include any personal information such as the e-mail address or the phone number.', 'hiv placements are not available if the following apply:   applies to this position,    exempt status applies to this position,    knowledge and skills evaluation:   see also: research, training and skills]']\"\n","2684\tCooperVision\tNY\t\"[\"\"it is against the law to discriminate against employees or applicants because of race, color, religion, sex, sexual orientation, national origin, age, marital status, or sexual orientation.\"\", 'law enforcement officials may use this law enforcement official's data to investigate crimes committed while these personnel were working for the law enforcement agency.', 'at the time of the incident reporting, no one from outside of the agency was available.,    for more information see:,    https:  www.patreon.com/atlanta   follow atlanta   twitter @atlanta,    atlanta,    laura.commansguide,    or https:  www.acasguide.com,    newsarounds team,    they support the operations group in developing new algorithms, data, and systems that will drive value for the community.,    a preliminary analysis of the candidates has identified these algorithms as a logical next step, with an ideal candidate likely to possess at least one of the following technical skills:   supervised learning: regression classification, linear algebra linear probability multivariate statistical sampling, simulation or other statistical analysis, multi-process algorithmic clustering, natural language processing, computational vision \\'ability to work effectively in a team environment using advanced modeling techniques, including exploratory analysis techniques, large-scale data mining, tree-based engineering, data pipelines, and predictive modeling.,    ability to work effectively in a collaborative work environment using analytical methods such as supervised testing, minder, inference, clustering, neuralnet, or monte d'Este.', 'experience programming in languages such as python, java, scala, c++, or c++.', 'preferred language: 蝡心中文 (zh) or    https:  www.acasguide.com,    why work at atlanta?,    as part of a team that can jointly explore and solve some of the most challenging and complex problems in blockchain and smart contract, we’re growing.', 'we are expanding out of a start-up, blockchain.in, which is considered a start-up at the beginning of a career.,    as a data scientist at atlanta, you’ll play a key role in research\n","\n","[810 | 1988.77] loss=0.06 avg=0.48\n","[820 | 2012.70] loss=0.08 avg=0.47\n","[830 | 2036.57] loss=0.05 avg=0.46\n","[840 | 2060.36] loss=0.06 avg=0.45\n","[850 | 2084.15] loss=0.06 avg=0.45\n","[860 | 2107.94] loss=0.04 avg=0.44\n","[870 | 2131.72] loss=0.06 avg=0.43\n","[880 | 2155.52] loss=0.06 avg=0.43\n","[890 | 2179.47] loss=0.04 avg=0.42\n","[900 | 2203.35] loss=0.04 avg=0.41\n","Saving checkpoint/run3/model-900\n","[910 | 2229.35] loss=0.05 avg=0.41\n","[920 | 2253.20] loss=0.07 avg=0.40\n","[930 | 2277.08] loss=0.05 avg=0.40\n","[940 | 2300.93] loss=0.05 avg=0.39\n","[950 | 2324.74] loss=0.05 avg=0.39\n","[960 | 2348.49] loss=0.06 avg=0.38\n","[970 | 2372.27] loss=0.05 avg=0.37\n","[980 | 2396.17] loss=0.05 avg=0.37\n","[990 | 2420.11] loss=0.03 avg=0.36\n","[1000 | 2443.98] loss=0.05 avg=0.36\n","Saving checkpoint/run3/model-1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VHdTL8NDbAh3"},"source":["gpt2.copy_checkpoint_to_gdrive(run_name='run3')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQJgV_b4bmzd"},"source":["You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."]},{"cell_type":"markdown","metadata":{"id":"pel-uBULXO2L"},"source":["## Load a Trained Model Checkpoint\n","\n"]},{"cell_type":"code","metadata":{"id":"DCcx5u7sbPTD"},"source":["gpt2.copy_checkpoint_from_gdrive(run_name='run3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fxL77nvAMAX","colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"status":"error","timestamp":1617287191089,"user_tz":-330,"elapsed":33495,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"1861e733-98ac-4849-ee5e-055e9cd8c656"},"source":["sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, run_name='run3')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-017155e9a4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, checkpoint, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'latest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ClJwpF_ACONp"},"source":["## Generate Text From The Trained Model\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RNY6RBI9LmL","executionInfo":{"status":"ok","timestamp":1616767450514,"user_tz":-330,"elapsed":21079,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"15768e8c-9428-4ba8-f618-230c6c2b59d5"},"source":["gpt2.generate(sess, run_name='run3')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["”   complement state-of-the-art data science tools and technologies with cutting-edge tools and technologies to support business use cases and deliver meaningful impact . , title: , extract value (d) from data (')    use d3.js or other data-driven analytic engine to create dashboards to monitor health status and customer behavior datakits deliver analytical results performing data-driven analyses using data that hasn’s already been collected   proactively analyze and fix defects in data or deliver insights to internal and external customers to validate data-driven aspects of processes   help business users to improve their analytical processes and recommend changes to the business to ensure smooth implementation   working with a board of directors of their choosing to help business “execute without self-fulfilling promises   , bachelors in a quantitative field, preferred; or equivalent experience in a quantitative discipline, preferred.', 'industry knowledge   , demonstrated experience with google analytics or other web analytics toolbox; or equivalent experience in a quantitative discipline, preferred.', 'industry knowledge   , demonstrated experience with google analytics or other web analytics toolbox; or equivalent experience in a quantitative discipline, preferred.', ', offer to selected candidate will be made contingent on the results of applicable background checks.', ', offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.', ', relocation assistance is available.', ', primary accountabilities   , creates business application and modeling framework, for new datasets, and discovers insights and relationships from large complex datasets through investigative research using advanced mathematical and or statistical techniques.', 'explores data using a variety of advanced statistical techniques to proactively identify relationships, create insights and answer business questions or guide future model development.', 'build the hypothesis, identify research data attributes and determine best approach to address business issues.', 'combines business acumen with mathematical capabilities to build complex predictive models to support business objectives.', 'builds complex programs for running mathematical or statistical tests on data and for understanding complex relationships across attributes.', 'incorporates findings and provides industry and competitor insights as part of model development and enhancement.', 'researches and maintains awareness of industry best practices and business strategies.', 'brings new and innovative ideas and approaches to develop business solutions.', 'monitors industry and competitor trends to determine potential impact to predictive models.', 'identifies, leverages and develops expertise in emerging technologies, open source tools, and harnesses new techniques (e.g., machine learning).', 'networks with and contributes thought leadership to the broader external analytics community.', 'builds awareness of leading techniques, tools, and data resources.', 'assists with complex research or analytics projects related to large, complex business initiatives.', 'interacts with company senior leadership to inform on industry trends and emerging research topics.', 'serves as internal expert for new areas of analytics exploration.', ', incorporates findings and provides industry and competitor insights as part of model development and enhancement.', ', stay connected: join our talent community!   ]']\n","1194\tDropbox\tCA\t['the team applies advanced analytics, data science, and engineering to develop cutting edge analytical solutions utilizing the latest technologies and techniques.', 'the output of our work is integrated into the godaddy ecosystem and leveraged by teams in our marketing, product development, operations, and care organizations.', 'our work helps shape how the firm goes to market.,    we are seeking team members that are passionate, extraordinary, analytical, strategic, and inquisitive.', 'the successful candidate will be an “out of the box” thinker, taking innovative and creative approaches to solving complex problems.', 'the right person will raise the profile and excellence of our entire team.', 'if this is you, we want to hear your story!', 'come join our extraordinary team and help us make our customers’ big ideas come to life!,    responsibilities:,    our team manages a wide portfolio of customer insights and value levers.', 'on the analytics team, you will find leadership opportunities in these areas:,    drive customer value growth through modeled insights which improve business results in product purchase, usage, renewal retention, product development, and customer engagement   build solutions that drive channel messaging excellence   uncover key customer segmentation and associated behavioral analyses including customer…   sentiment and loyalty   preferences   price sensitivity   venture profiles   engagement intent   develop methodologies to predict customer behavior and prescribe best course of action   execute all stages of the modeling process: data gathering, feature development, model selection, validation, presentation and socialization   advanced visualization of results over high resolution and web-quality   support marketing staff in problem definition and process redesign   support technical solution teams in problem definition and process redesign   assist in solution development and metric measurement of solution mass produced   support marketing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8DKMc0fiej4N"},"source":["text = gpt2.generate(sess,\n","              length=250,\n","              temperature=0.7,\n","              return_as_list=True\n","              prefix=\"Express Scripts\tMO\",\n","              nsamples=2\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuirqIUQiDSR"},"source":["gpt2.generate(sess,\n","              length=250,\n","              temperature=0.7,\n","              prefix=\"Rice University\tTX\",\n","              nsamples=2\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3y2QLEh3ek1z","executionInfo":{"status":"ok","timestamp":1616828356116,"user_tz":-330,"elapsed":8359,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"c5e1a0c5-2b19-48f3-f70a-45bbd72ccf66"},"source":["gpt2.generate(sess,\n","              length=250,\n","              temperature=0.7,\n","              prefix=\"Rice University\tTX\",\n","              nsamples=2\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Rice University\tTX\t\"['[csnl:4581 grantee, exact type of position, compensation, plan to train your future top dog, why in dl, how it will help you, what the vision is, and how you will contribute.', 'your life.', 'your art.', 'your country.', 'for us, your life is about you.', 'come join our team and become a world-class individual today.', 'for more information, visit www.csnl.com,    our values us, our us is about.', 'we is better.', 'we is better.', 'we is better.', 'if you are.,    job title: individual with limited resources capacity: able to perform essential functions through the use of limited resources.\"\", 'location: region: anywhere in the world   primary location: edinbrussell.', 'fremont, mn: elsinore california 97203   position description,    in this role you will perform basic and advanced analytics in clinical care, research, and analytics, using digital technologies and blockchain.', 'you will use these analytics to improve patient care and understand outcomes,\n","====================\n","Rice University\tTX\t\"[\"\"the team leverages a modern big-data and microservice-based technology stack for our end-to-end data processing, analysis, api, and web application – to provide our users with the insights they need to be successful.,    responsibilities,    technical contributor as a full-stack developer in a small, cross-functional development team, focused on providing data analytics as a service to internal and external hp customers.\"\", 'develop unit of data solutions in big data environment.,    technical contributor to data stores that store data in big data technologies such as r, spark, scala, and hadoop.', 'build enterprise-wide data products using a modern big-data and cloud-connected technology stack every day.', 'you will be a trusted partner in the development of these solutions, helping hp to build products that are sustainable, high-value, affordable, sustainable, and trustworthy.', 'build highly scalable big-data and cloud-connected technologies for the enterprise.', 'you will support customers on project scope, by providing them the tools they need to build their own projects.', 'build enterprise-wide data solutions using a modern big-data and cloud-connected technology stack\n","====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzW5k2vxjGVD","executionInfo":{"status":"ok","timestamp":1616829846550,"user_tz":-330,"elapsed":60268,"user":{"displayName":"CHANDRIKA","photoUrl":"","userId":"13931345100314558168"}},"outputId":"eb08b806-4227-4b3a-ea92-615a689df5a3"},"source":["gpt2.generate(sess,\n","              length=250,\n","              temperature=0.7,\n","              prefix=\"Zumper CA\",\n","              nsamples=2\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Zumper CA-2-2,    other locations:]']\n","1939\tdata scientist\tai data mining machine learning r tableau splunk python\t['[do you want to work on machine learning problems that require a quantitative approach?', 'you are passionate about finding insights, building predictive models, and building predictive models to solve complex business problems.', 'you are passionate about solving challenging problems with your own hands-on experience.', 'you love learning new things about the environment you work in.', 'you love taking risks, being a self-starter, and have a passion for learning new things about the environment.', 'you have a strong sense of curiosity and a desire to understand the business and the challenges that come with owning a business.', 'you have a strong sense of technical skill and a desire to help people find meaning in a process   continuous improvement mindset.]']\n","1941\tdata scientist\thive machine learning r sql tableau natural language processing data analysis python\t['[   you are a self-starter who is looking to broaden and grow your skillset with the open source community.', 'you are an open-minded individual with an entrepreneurial mindset.', 'you like to explore,\n","====================\n","Zumper CA-1) to resolve technical issues:,    test and deploy code for security vulnerabilities:,    use solr, rds and other tools to automate testing and deployment of processes and applications:,    use sql, python, r, and or similar scripting language to design and deploy software applications in a production environment,    work with software engineering to improve processes and tools,    identify and understand problems and opportunities,    design and build software applications,    conduct testing and development of software application applications,    train and encourage other testers to participate in the same as part of the project,    test software applications and features,    conduct standard testing of applications,    work with other testing teams to resolve technical issues,    use solr, rds and other tools to automate testing and deployment of processes and applications,    use sql, python, r, and or similar scripting language to design and deploy software applications in a production environment,    work with software engineering to improve processes and tools,    use solr, rds and other tools to automate testing and deployment of processes and applications,    work with other\n","====================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zjjEN2Tafhl2"},"source":["For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n","\n","You can rerun the cells as many times as you want for even more generated texts!"]},{"cell_type":"code","metadata":{"id":"Fa6p6arifSL0"},"source":["gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=500,\n","                      temperature=0.7,\n","                      nsamples=100,\n","                      batch_size=20\n","                      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-LRex8lfv1g"},"source":["# may have to run twice to get file to download\n","files.download(gen_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQAN3M6RT7Kj"},"source":["## Generate Text From The Pretrained Model\n","\n","If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n","\n","This is currently the only way to generate text from the 774M or 1558M models with this notebook."]},{"cell_type":"code","metadata":{"id":"hsUd_jHgUZnD"},"source":["model_name = \"774M\"\n","\n","gpt2.download_gpt2(model_name=model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAe4NpKNUj2C"},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.load_gpt2(sess, model_name=model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xInIZKaU104"},"source":["gpt2.generate(sess,\n","              model_name=model_name,\n","              prefix=\"The secret of life is\",\n","              length=100,\n","              temperature=0.7,\n","              top_p=0.9,\n","              nsamples=5,\n","              batch_size=5\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ig-KVgkCDCKD"},"source":["# Etcetera\n","\n","If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"]},{"cell_type":"code","metadata":{"id":"rIHiVP53FnsX"},"source":["!kill -9 -1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmTXWNUygS5E"},"source":["# LICENSE\n","\n","MIT License\n","\n","Copyright (c) 2019 Max Woolf\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE."]}]}